\section{One-Time Pad}
\index{One-Time Pad}
The one-time pad scheme was proposed in 1917 by Major J. Mauborgne and G. Vernam. The classic one-time pad consists of a set of random keys, each equal in size to the message being sent and used only once.

Let's assume we want to encrypt a message in some language (e.g., in English). The number of symbols (letters) used in the alphabet is denoted as $X$. For the English language (without punctuation and case sensitivity) $X = 26$. Then we assign a number $c$ to each symbol of the language such that $0 \le c < X$. For example, in English, we can write
\begin{equation}
\begin{array}{c}
A \rightarrow 0 \\
B \rightarrow 1 \\
\dots \\
Z \rightarrow 25 \\
\end{array}
\nonumber
\end{equation}
  
The encryption procedure \eqref{eqPart3CryptoEncryptClass} is described by the following expression
\begin{equation}
E_{K_i}\left(P_i\right) = P_i + K_i \mod X = C_i,
\label{eqPart3CryptoEncryptVernam}
\end{equation}
where $i$ is the index of the encrypted symbol.

The decryption procedure \eqref{eqPart3CryptoDeEncryptClass} is described by the following expression
\begin{equation}
D_{K_i}\left(C_i\right) = C_i - K_i \mod X = P_i,
\label{eqPart3CryptoDeEncryptVernam}
\end{equation}
where $i$ is the index of the encrypted symbol.

This procedure easily generalizes to the case of binary data, where instead of modular addition, the XOR operation ($a \oplus b$) is used for both encryption and decryption:
\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
$a$ & $b$ & $a \oplus b$ \\ \hline
0  & 0 & 0 \\
0  & 1 & 1 \\
1  & 0 & 1 \\
1  & 1 & 0 \\ \hline
\end{tabular}
\caption{XOR $a \oplus b$}
\label{tblXOR}
\end{table}

Claude Shannon showed \cite{bShenonCrypto} that if the key is truly random, has the same length as the original message, and is not reused, the proposed one-time pad scheme is perfectly secure.

According to Shannon, perfect security can be defined as follows.
\begin{definition}
A cipher $\left(E, D\right)$ is perfectly secure if for any two messages of the same length $m_0$ and $m_1$, some ciphertext $c$, and key $k \leftarrow_R K$, the probabilities that the original text is $m_0$ or $m_1$ are equal:
\begin{equation}
P\left(E\left(m_0, k\right) = c \right) = 
P\left(E\left(m_1, k\right) = c \right)
\nonumber
\end{equation}
\end{definition}
Rephrasing this definition, one can say that from the initial statistics of the ciphertext, no information about the original message can be obtained.

\begin{theorem}[Cryptographic Strength of the One-Time Pad]
The one-time pad scheme is perfectly secure.
\end{theorem}

\begin{proof}
Let $\left|K\right|$ be the number of all possible keys of length $l$. Where $l$ is also the length of the original messages: $\left|m_{0,1}\right| = l$. Since the key used to encrypt the message is uniquely defined:
\begin{equation}
k_{0,1} = c \oplus m_{0,1},
\nonumber
\end{equation} 
we obtain for the probabilities
\begin{equation}
P\left(E\left(m_0, k\right) = c \right) = 
P\left(E\left(m_1, k\right) = c \right) = 
\frac{1}{\left|K\right|}.
\nonumber
\end{equation}
\end{proof}

\section{Problems of Classical Cryptography}

If there exists a perfectly secure cryptographic system (one-time pad), what is wrong with classical cryptography? The problem lies in obtaining keys that meet the requirements of the one-time pad (the key length is equal to the message length, the key consists of random data, and it is not reused) and transferring these keys to Bob and Alice.

Problems arise both at the stage of key generation,\footnote{obtaining large sequences of random numbers is a non-trivial mathematical task} and at the stage of transferring these keys.

In classical cryptography, so-called public key algorithms are used to transfer keys. There are several key exchange protocols based on public key cryptographic systems. They are all based on the existence of two keys: the first, called the public key, is used only for encryption, and the second, the private key, is used for decryption. To obtain the private key from the public one, a complex mathematical operation is required. For example, the security of one of the most popular public key systems—RSA (see \ref {AddRSA})—\index{RSA algorithm} is based on the difficulty of factoring\footnote{decomposition into prime factors} large numbers.

A key distribution protocol scheme based on public key cryptography can be described as follows. In the first stage, Alice creates public and private keys and sends the first to Bob. Bob, for his part, creates the key that Alice and Bob want to have (which needs to be distributed). This key is encrypted (for example, via RSA) using Alice's public key and sent to her. Alice, receiving this encrypted key, can decrypt it with her private key.

If an intruder (Eve) wants to learn the transmitted key, she must solve a complex mathematical problem of factoring large numbers. It is believed, but not proven, that the complexity of factoring grows exponentially with the number of digits in the number \cite{bPhisQuantInfo}.\footnote{The fastest known algorithm solves the factoring problem for number $N$ in time approximately $O\left(exp\left(log^{\frac{1}{3}}N\left(log \, log N\right)^{\frac{2}{3}}\right)\right)$.} Thus, as the number of digits increases, the task quickly becomes unsolvable.

This scheme has several problems. The first is that the complexity of factoring is not proven. Moreover, there are algorithms for quantum computers—Shor's algorithm (see \ref {Part4QuantCompShor})—that solve the factoring problem for number $N$ in time $O\left(log N\right)$, i.e., on the order of the number of digits in $N$. Thus, at the moment a quantum computer is built, all systems based on RSA \index{RSA algorithm} will become obsolete.