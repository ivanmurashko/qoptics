%% -*- coding:utf-8 -*-
\subsection{Random Variable. Distribution Functions}

\begin{definition}[Random Variable]
\label{def:random_variable}
If $\Omega$ is the sample space (see definition \autoref{def:events_set}), then 
a mapping $X: \Omega \to \mathbb{R}$ is called a random
variable. 
\end{definition}

It is worth noting that if a random variable is denoted by 
an uppercase letter $X$, then a specific realization is denoted by a lowercase letter
$x$, i.e. if $\omega \in \Omega$, then $x = X(\omega)$. 
In particular, for discrete random variables, to each realization
$x = X(\omega)$ one can assign some probability
$P\left(x\right) = P\left(\omega\right)$. 
Otherwise, distribution functions are used.

\begin{definition}[Distribution Function of a Random Variable]
The distribution function $F_X$ of a random variable $X$ is a function
$F_X : \mathbb{R} \to [0,1]$ defined by the formula
\[
F_X \left(x\right) = P\left(X \le x\right),
\]
i.e. the distribution function at argument $x$ equals the probability of those
events $\omega \in \Omega$ for which
$X(\omega) \le x$.
\end{definition}

\begin{definition}[Probability Density Function of a Random Variable]
In the case of continuous random variables, it makes sense to speak about the
probability density function $f_X\left(x\right)$, 
which is defined by the following expression 
\[
f_X\left(x\right) = 
\frac{d F_X\left(x\right)}{d x}
\]
\end{definition}

\begin{definition}[Expectation of a Random Variable]
For a discrete random variable, i.e. if the set of elementary
outcomes is countable, the expectation $E\left(X\right)$ of the random variable $X$
is defined by the following relation
\[
E\left(X\right) = \sum_\omega p_\omega \cdot X(\omega) = 
\sum_x p_x \cdot x,
\] 
where the summation is over all possible elementary outcomes
$\omega$, $x = X(\omega)$, $p_\omega = p_{X(\omega)} = p_x$.
For continuous random variables, this relation transforms into 
\[
E\left(X\right) = \int_{-\infty}^\infty x \cdot f_X\left(x\right) dx.
\]
\end{definition}


TBD
